{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T03:05:36.902356Z",
     "start_time": "2019-04-22T03:05:36.896731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/xijunli/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\n",
      "/Users/xijunli/Desktop/KDDCup2019/jupyter\n",
      "/Users/xijunli/Desktop/KDDCup2019/jupyter\n",
      "/Users/xijunli/Desktop/KDDCup2019\n",
      "/Users/xijunli/Desktop/KDDCup2019/jupyter\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.argv[0])\n",
    "import os\n",
    "print (os.getcwd())#获得当前工作目录\n",
    "print (os.path.abspath('.'))#获得当前工作目录\n",
    "print (os.path.abspath('..'))#获得当前工作目录的父目录\n",
    "print (os.path.abspath(os.curdir))#获得当前工作目录\n",
    "# print(os.path.abspath('..')+'/src/sample_code_submission')\n",
    "# os.chdir(os.path.abspath('..')+'/src/sample_code_submission')\n",
    "# os.path.abspath('..')+'/src/sample_code_submission'\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T12:33:04.717408Z",
     "start_time": "2019-04-25T12:33:04.701279Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  [04-25 20:33:04] Import Model\n"
     ]
    }
   ],
   "source": [
    "# pylint: disable=wrong-import-order, wrong-import-position, import-error\n",
    "# pylint: disable=missing-docstring\n",
    "import base64\n",
    "from datetime import datetime\n",
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "\n",
    "#  os.system(\"pip3 install cryptography\")\n",
    "\n",
    "\n",
    "def mprint(msg):\n",
    "    \"\"\"info\"\"\"\n",
    "    cur_time = datetime.now().strftime('%m-%d %H:%M:%S')\n",
    "    print(f\"INFO  [{cur_time}] {msg}\")\n",
    "\n",
    "\n",
    "mprint(\"Import Model\")\n",
    "\n",
    "import json\n",
    "import signal\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "TYPE_MAP = {\n",
    "    'time': str,\n",
    "    'cat': str,\n",
    "    'multi-cat': str,\n",
    "    'num': np.float64\n",
    "}\n",
    "\n",
    "\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self.duration = 0\n",
    "        self.total = None\n",
    "        self.remain = None\n",
    "        self.exec = None\n",
    "\n",
    "    def set(self, time_budget):\n",
    "        self.total = time_budget\n",
    "        self.remain = time_budget\n",
    "        self.exec = 0\n",
    "\n",
    "    @contextmanager\n",
    "    def time_limit(self, pname):\n",
    "        def signal_handler(signum, frame):\n",
    "            raise TimeoutException(\"Timed out!\")\n",
    "        signal.signal(signal.SIGALRM, signal_handler)\n",
    "        signal.alarm(self.remain)\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            exec_time = time.time() - start_time\n",
    "            signal.alarm(0)\n",
    "            self.exec += exec_time\n",
    "            self.duration += exec_time\n",
    "            remain_time = math.ceil(self.total - self.exec)\n",
    "            self.remain = remain_time\n",
    "\n",
    "            mprint(f'{pname} success, time spent so far {self.exec} sec')\n",
    "\n",
    "\n",
    "def read_train(datapath, info):\n",
    "    train_data = {}\n",
    "    for table_name, columns in info['tables'].items():\n",
    "        mprint(f'Table name: {table_name}')\n",
    "\n",
    "        table_dtype = {key: TYPE_MAP[val] for key, val in columns.items()}\n",
    "\n",
    "        if table_name == 'main':\n",
    "            table_path = join(datapath, 'train', 'main_train.data')\n",
    "        else:\n",
    "            table_path = join(datapath, 'train', f'{table_name}.data')\n",
    "\n",
    "        date_list = [key for key, val in columns.items() if val == 'time']\n",
    "\n",
    "        train_data[table_name] = pd.read_csv(\n",
    "            table_path, sep='\\t', dtype=table_dtype, parse_dates=date_list,\n",
    "            date_parser=lambda millisecs: millisecs if np.isnan(\n",
    "                float(millisecs)) else datetime.fromtimestamp(\n",
    "                    float(millisecs)/1000))\n",
    "\n",
    "    # get train label\n",
    "    train_label = pd.read_csv(\n",
    "        join(datapath, 'train', 'main_train.solution'))['label']\n",
    "    return train_data, train_label\n",
    "\n",
    "\n",
    "def read_info(datapath):\n",
    "    mprint('Read info')\n",
    "    with open(join(datapath, 'train', 'info.json'), 'r') as info_fp:\n",
    "        info = json.load(info_fp)\n",
    "    mprint(f'Time budget for this task is {info[\"time_budget\"]} sec')\n",
    "    return info\n",
    "\n",
    "\n",
    "def read_test(datapath, info):\n",
    "    # get test data\n",
    "    main_columns = info['tables']['main']\n",
    "    table_dtype = {key: TYPE_MAP[val] for key, val in main_columns.items()}\n",
    "\n",
    "    table_path = join(datapath, 'test', 'main_test.data')\n",
    "\n",
    "    date_list = [key for key, val in main_columns.items() if val == 'time']\n",
    "\n",
    "    test_data = pd.read_csv(\n",
    "        table_path, sep='\\t', dtype=table_dtype, parse_dates=date_list,\n",
    "        date_parser=lambda millisecs: millisecs if np.isnan(\n",
    "            float(millisecs)) else datetime.fromtimestamp(\n",
    "                float(millisecs) / 1000))\n",
    "    return test_data\n",
    "\n",
    "\n",
    "def write_predict(output_dir, dataname, prediction):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    prediction.rename('label', inplace=True)\n",
    "    prediction.to_csv(\n",
    "        join(output_dir, f'{dataname}.predict'), index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T12:33:05.161767Z",
     "start_time": "2019-04-25T12:33:04.719546Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  [04-25 20:33:04] Datanames: ['K', 'L']\n",
      "INFO  [04-25 20:33:04] Read data: K\n",
      "INFO  [04-25 20:33:04] Read info\n",
      "INFO  [04-25 20:33:04] Time budget for this task is 6000 sec\n",
      "INFO  [04-25 20:33:04] Table name: main\n",
      "INFO  [04-25 20:33:04] Table name: table_1\n",
      "INFO  [04-25 20:33:05] Table name: table_2\n",
      "INFO  [04-25 20:33:05] Table name: table_3\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = os.path.abspath('/Users/xijunli/Desktop/KDDCup2019/starting_kit_0401/')\n",
    "DIRS = {\n",
    "        'input': join(ROOT_DIR, 'sample_data'),\n",
    "        'output': join(ROOT_DIR, 'sample_predictions'),\n",
    "        'program': join(ROOT_DIR, 'ingestion_program'),\n",
    "        'submission': join(ROOT_DIR, 'sample_code_submission')\n",
    "}\n",
    "\n",
    "datanames = sorted(os.listdir(DIRS['input']))\n",
    "mprint(f'Datanames: {datanames}')\n",
    "timer = Timer()\n",
    "dataname = datanames[0]\n",
    "predictions = {}\n",
    "mprint(f'Read data: {dataname}')\n",
    "datapath = join(DIRS['input'], dataname)\n",
    "info = read_info(datapath)\n",
    "timer.set(info['time_budget'])\n",
    "train_data, train_label = read_train(datapath, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T12:33:27.089447Z",
     "start_time": "2019-04-25T12:33:27.051694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/xijunli/Desktop/KDDCup2019/src/sample_code_submission\n"
     ]
    }
   ],
   "source": [
    "os.chdir(os.path.abspath('..')+'/sample_code_submission/')\n",
    "print(os.path.abspath('.'))\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from automl import predict, train, validate\n",
    "from CONSTANT import MAIN_TABLE_NAME\n",
    "from merge import merge_table\n",
    "from preprocess import clean_df, clean_tables, feature_engineer\n",
    "from util import Config, log, show_dataframe, timeit\n",
    "\n",
    "config = Config(info)\n",
    "tables = copy.deepcopy(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T12:33:31.333625Z",
     "start_time": "2019-04-25T12:33:31.277856Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----cleaning table main\n",
      "\n",
      "----Start [clean_df]:\n",
      "--------Start [fillna]:\n",
      "--------End   [fillna]. Time elapsed: 0.01 sec.\n",
      "----End   [clean_df]. Time elapsed: 0.01 sec.\n",
      "----cleaning table table_1\n",
      "\n",
      "----Start [clean_df]:\n",
      "--------Start [fillna]:\n",
      "--------End   [fillna]. Time elapsed: 0.02 sec.\n",
      "----End   [clean_df]. Time elapsed: 0.02 sec.\n",
      "----cleaning table table_2\n",
      "\n",
      "----Start [clean_df]:\n",
      "--------Start [fillna]:\n",
      "--------End   [fillna]. Time elapsed: 0.01 sec.\n",
      "----End   [clean_df]. Time elapsed: 0.01 sec.\n",
      "----cleaning table table_3\n",
      "\n",
      "----Start [clean_df]:\n",
      "--------Start [fillna]:\n",
      "--------End   [fillna]. Time elapsed: 0.00 sec.\n",
      "----End   [clean_df]. Time elapsed: 0.00 sec.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import CONSTANT\n",
    "from util import Config, Timer, log, timeit\n",
    "\n",
    "NUM_OP = [np.std, np.mean]\n",
    "\n",
    "for tname in tables:\n",
    "    log(f\"cleaning table {tname}\")\n",
    "    clean_df(tables[tname])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X = merge_table(Xs, self.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T12:33:34.530915Z",
     "start_time": "2019-04-25T12:33:34.455020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----enter main\n",
      "----enter table_1\n",
      "----leave table_1\n",
      "----join main <--many_to_one--nt table_1\n",
      "\n",
      "----Start [join]:\n",
      "----End   [join]. Time elapsed: 0.03 sec.\n",
      "----enter table_2\n",
      "----leave table_2\n",
      "----join main <--many_to_one--nt table_2\n",
      "\n",
      "----Start [join]:\n",
      "----End   [join]. Time elapsed: 0.02 sec.\n",
      "----enter table_3\n",
      "----leave table_3\n",
      "----join main <--many_to_one--nt table_3\n",
      "\n",
      "----Start [join]:\n",
      "----End   [join]. Time elapsed: 0.01 sec.\n",
      "----leave main\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import CONSTANT\n",
    "from util import Config, Timer, log, timeit\n",
    "from merge import bfs, dfs\n",
    "# def merge_table(tables, config):\n",
    "\n",
    "graph = defaultdict(list)\n",
    "for rel in config['relations']:\n",
    "    ta = rel['table_A']\n",
    "    tb = rel['table_B']\n",
    "    graph[ta].append({\n",
    "            \"to\": tb,\n",
    "            \"key\": rel['key'],\n",
    "            \"type\": rel['type']\n",
    "    })\n",
    "    graph[tb].append({\n",
    "            \"to\": ta,\n",
    "            \"key\": rel['key'],\n",
    "            \"type\": '_'.join(rel['type'].split('_')[::-1])\n",
    "    })\n",
    "bfs(CONSTANT.MAIN_TABLE_NAME, graph, config['tables'])\n",
    "X = dfs(CONSTANT.MAIN_TABLE_NAME, config, tables, graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T12:33:51.408721Z",
     "start_time": "2019-04-25T12:33:51.228162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----Start [clean_df]:\n",
      "--------Start [fillna]:\n",
      "--------End   [fillna]. Time elapsed: 0.03 sec.\n",
      "----End   [clean_df]. Time elapsed: 0.03 sec.\n",
      "\n",
      "----Start [feature_engineer]:\n",
      "--------Start [transform_categorical_hash]:\n",
      "--------End   [transform_categorical_hash]. Time elapsed: 0.12 sec.\n",
      "\n",
      "--------Start [transform_datetime]:\n",
      "--------End   [transform_datetime]. Time elapsed: 0.01 sec.\n",
      "----End   [feature_engineer]. Time elapsed: 0.13 sec.\n"
     ]
    }
   ],
   "source": [
    "clean_df(X)\n",
    "X_fe = copy.deepcopy(X)\n",
    "feature_engineer(X_fe,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = pd.DataFrame({'key': ['K0', 'K1', 'K1', 'K3', 'K1', 'K1', 'K3'],\n",
    "                      'A': [1,2,3,4,5,6,7],\n",
    "                      'B': [3,5,2,5,2,3,2]})\n",
    " \n",
    "\n",
    "left_rolling = left.groupby([\"key\"]).rolling(3).agg({\"A\": \"sum\",\n",
    "                                     \"B\": \"sum\"}) \\\n",
    "    .reset_index(0, drop=True) \n",
    "\n",
    "left.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T12:31:16.757813Z",
     "start_time": "2019-04-25T12:31:16.754439Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "import hyperopt\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hyperopt import STATUS_OK, Trials, hp, space_eval, tpe\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from util import Config, log, timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T12:37:22.537165Z",
     "start_time": "2019-04-25T12:37:20.911393Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  7.65it/s, best loss: -0.6957329351390847]\n",
      "----auc = 0.6957 {'bagging_fraction': 0.8, 'bagging_freq': 22, 'feature_fraction': 0.5, 'learning_rate': 0.11643157545599682, 'max_depth': 6, 'min_child_weight': 9.195866326847561, 'num_leaves': 145, 'reg_alpha': 0.8629175589887947, 'reg_lambda': 1.935131592856683}\n"
     ]
    }
   ],
   "source": [
    "def data_sample(X: pd.DataFrame, y: pd.Series, nrows: int=5000):\n",
    "    # -> (pd.DataFrame, pd.Series):\n",
    "    if len(X) > nrows:\n",
    "        X_sample = X.sample(nrows, random_state=1)\n",
    "        y_sample = y[X_sample.index]\n",
    "    else:\n",
    "        X_sample = X\n",
    "        y_sample = y\n",
    "\n",
    "    return X_sample, y_sample\n",
    "\n",
    "def data_split(X: pd.DataFrame, y: pd.Series, test_size: float=0.2):\n",
    "    #  -> (pd.DataFrame, pd.Series, pd.DataFrame, pd.Series):\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=1)\n",
    "\n",
    "params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"auc\",\n",
    "        \"verbosity\": -1,\n",
    "        \"seed\": 1,\n",
    "        \"num_threads\": 4\n",
    "    }\n",
    "\n",
    "X_sample, y_sample = data_sample(X_fe, train_label, 30000)\n",
    "\n",
    "X_train, X_val, y_train, y_val = data_split(X_sample, y_sample, test_size=0.5)\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "valid_data = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "space = {\n",
    "        \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.01), np.log(0.5)),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", [-1, 2, 3, 4, 5, 6]),\n",
    "        \"num_leaves\": hp.choice(\"num_leaves\", np.linspace(10, 200, 50, dtype=int)),\n",
    "        \"feature_fraction\": hp.quniform(\"feature_fraction\", 0.5, 1.0, 0.1),\n",
    "        \"bagging_fraction\": hp.quniform(\"bagging_fraction\", 0.5, 1.0, 0.1),\n",
    "        \"bagging_freq\": hp.choice(\"bagging_freq\", np.linspace(0, 50, 10, dtype=int)),\n",
    "        \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 2),\n",
    "        \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 2),\n",
    "        \"min_child_weight\": hp.uniform('min_child_weight', 0.5, 10),\n",
    "}\n",
    "\n",
    "def objective(hyperparams):\n",
    "    model = lgb.train({**params, **hyperparams}, train_data, 300,\n",
    "                          valid_data, early_stopping_rounds=30, verbose_eval=0)\n",
    "\n",
    "    score = model.best_score[\"valid_0\"][params[\"metric\"]]\n",
    "        # in classification, less is better\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "trials = Trials()\n",
    "best = hyperopt.fmin(fn=objective, space=space, trials=trials,\n",
    "                         algo=tpe.suggest, max_evals=10, verbose=1,\n",
    "                         rstate=np.random.RandomState(1))\n",
    "\n",
    "hyperparams = space_eval(space, best)\n",
    "log(f\"auc = {-trials.best_trial['result']['loss']:0.4f} {hyperparams}\")\n",
    "\n",
    "\n",
    "# hyperparams = hyperopt_lightgbm(X_sample, y_sample, params, config)\n",
    "\n",
    "# X_train, X_val, y_train, y_val = data_split(X, y, 0.1)\n",
    "# train_data = lgb.Dataset(X_train, label=y_train)\n",
    "# valid_data = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "# config[\"model\"] = lgb.train({**params, **hyperparams},\n",
    "#                                 train_data,\n",
    "#                                 500,\n",
    "#                                 valid_data,\n",
    "#                                 early_stopping_rounds=30,\n",
    "#                                 verbose_eval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing feature tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T00:30:53.066069Z",
     "start_time": "2019-04-30T00:30:53.041275Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xijunli/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: Creating a DatetimeIndex by passing range endpoints is deprecated.  Use `pandas.date_range` instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Entity: observations\n",
       "  Variables:\n",
       "    obs_id (dtype: index)\n",
       "    first_timesteps_time (dtype: datetime_time_index)\n",
       "  Shape:\n",
       "    (Rows: 3, Columns: 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from featuretools.primitives import Day, Percentile, CumMean, CumSum\n",
    "import featuretools as ft\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "timesteps = pd.DataFrame({'ts_id': range(12),\n",
    "                          'timestamp': pd.DatetimeIndex(start='1/1/2018', freq='1d', periods=12),\n",
    "                          'attr1': np.random.random(12),\n",
    "                          'obs_id': [1, 2, 3] * 4})\n",
    "# print(timesteps)\n",
    "\n",
    "entityset = ft.EntitySet(\"timeseries\")\n",
    "entityset.entity_from_dataframe(\"timesteps\",\n",
    "                                timesteps,\n",
    "                                index='ts_id',\n",
    "                                time_index='timestamp')\n",
    "entityset.normalize_entity(base_entity_id='timesteps',\n",
    "                           new_entity_id='observations',\n",
    "                           index='obs_id',\n",
    "                           make_time_index=True)\n",
    "\n",
    "entityset['observations']\n",
    "\n",
    "# # per timestep\n",
    "# cutoffs = timesteps[['ts_id', 'timestamp']]\n",
    "# feature_matrix, feature_list = ft.dfs(entityset=entityset,\n",
    "#                                       target_entity='timesteps',\n",
    "#                                       cutoff_time=cutoffs,\n",
    "#                                       trans_primitives=[Day, Percentile, CumMean, CumSum],\n",
    "#                                       agg_primitives=[])\n",
    "\n",
    "# entityset = ft.EntitySet(\"timeseries\")\n",
    "# entityset.entity_from_dataframe(\"timesteps\",\n",
    "#                                 timesteps,\n",
    "#                                 index='ts_id',\n",
    "#                                 time_index='timestamp')\n",
    "# entityset.normalize_entity(base_entity_id='timesteps',\n",
    "#                            new_entity_id='observations',\n",
    "#                            index='obs_id',\n",
    "#                            make_time_index=True)\n",
    "\n",
    "# # per timestep\n",
    "# cutoffs = timesteps[['ts_id', 'timestamp']]\n",
    "# feature_matrix, feature_list = ft.dfs(entityset=entityset,\n",
    "#                                       target_entity='timesteps',\n",
    "#                                       cutoff_time=cutoffs,\n",
    "#                                       trans_primitives=[Day, Percentile, CumMean, CumSum],\n",
    "#                                       agg_primitives=[])\n",
    "\n",
    "# feature_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
