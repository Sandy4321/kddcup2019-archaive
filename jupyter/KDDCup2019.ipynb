{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:46:52.663644Z",
     "start_time": "2019-05-31T08:46:52.658272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/xijunli/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\n",
      "/Users/xijunli/Desktop/KDDCup2019/src/sample_code_submission_submit\n",
      "/Users/xijunli/Desktop/KDDCup2019/src/sample_code_submission_submit\n",
      "/Users/xijunli/Desktop/KDDCup2019/src\n",
      "/Users/xijunli/Desktop/KDDCup2019/src/sample_code_submission_submit\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.argv[0])\n",
    "import os\n",
    "print (os.getcwd())#获得当前工作目录\n",
    "print (os.path.abspath('.'))#获得当前工作目录\n",
    "print (os.path.abspath('..'))#获得当前工作目录的父目录\n",
    "print (os.path.abspath(os.curdir))#获得当前工作目录\n",
    "# print(os.path.abspath('..')+'/src/sample_code_submission')\n",
    "# os.chdir(os.path.abspath('..')+'/src/sample_code_submission')\n",
    "# os.path.abspath('..')+'/src/sample_code_submission'\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:46:52.686945Z",
     "start_time": "2019-05-31T08:46:52.670490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  [05-31 16:46:52] Import Model\n"
     ]
    }
   ],
   "source": [
    "# pylint: disable=wrong-import-order, wrong-import-position, import-error\n",
    "# pylint: disable=missing-docstring\n",
    "import base64\n",
    "from datetime import datetime\n",
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "\n",
    "#  os.system(\"pip3 install cryptography\")\n",
    "\n",
    "\n",
    "def mprint(msg):\n",
    "    \"\"\"info\"\"\"\n",
    "    cur_time = datetime.now().strftime('%m-%d %H:%M:%S')\n",
    "    print(f\"INFO  [{cur_time}] {msg}\")\n",
    "\n",
    "\n",
    "mprint(\"Import Model\")\n",
    "\n",
    "import json\n",
    "import signal\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "TYPE_MAP = {\n",
    "    'time': str,\n",
    "    'cat': str,\n",
    "    'multi-cat': str,\n",
    "    'num': np.float64\n",
    "}\n",
    "\n",
    "\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self.duration = 0\n",
    "        self.total = None\n",
    "        self.remain = None\n",
    "        self.exec = None\n",
    "\n",
    "    def set(self, time_budget):\n",
    "        self.total = time_budget\n",
    "        self.remain = time_budget\n",
    "        self.exec = 0\n",
    "\n",
    "    @contextmanager\n",
    "    def time_limit(self, pname):\n",
    "        def signal_handler(signum, frame):\n",
    "            raise TimeoutException(\"Timed out!\")\n",
    "        signal.signal(signal.SIGALRM, signal_handler)\n",
    "        signal.alarm(self.remain)\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            exec_time = time.time() - start_time\n",
    "            signal.alarm(0)\n",
    "            self.exec += exec_time\n",
    "            self.duration += exec_time\n",
    "            remain_time = math.ceil(self.total - self.exec)\n",
    "            self.remain = remain_time\n",
    "\n",
    "            mprint(f'{pname} success, time spent so far {self.exec} sec')\n",
    "\n",
    "\n",
    "def read_train(datapath, info):\n",
    "    train_data = {}\n",
    "    for table_name, columns in info['tables'].items():\n",
    "        mprint(f'Table name: {table_name}')\n",
    "\n",
    "        table_dtype = {key: TYPE_MAP[val] for key, val in columns.items()}\n",
    "\n",
    "        if table_name == 'main':\n",
    "            table_path = join(datapath, 'train', 'main_train.data')\n",
    "        else:\n",
    "            table_path = join(datapath, 'train', f'{table_name}.data')\n",
    "\n",
    "        date_list = [key for key, val in columns.items() if val == 'time']\n",
    "\n",
    "        train_data[table_name] = pd.read_csv(\n",
    "            table_path, sep='\\t', dtype=table_dtype, parse_dates=date_list,\n",
    "            date_parser=lambda millisecs: millisecs if np.isnan(\n",
    "                float(millisecs)) else datetime.fromtimestamp(\n",
    "                    float(millisecs)/1000))\n",
    "\n",
    "    # get train label\n",
    "    train_label = pd.read_csv(\n",
    "        join(datapath, 'train', 'main_train.solution'))['label']\n",
    "    return train_data, train_label\n",
    "\n",
    "\n",
    "def read_info(datapath):\n",
    "    mprint('Read info')\n",
    "    with open(join(datapath, 'train', 'info.json'), 'r') as info_fp:\n",
    "        info = json.load(info_fp)\n",
    "    mprint(f'Time budget for this task is {info[\"time_budget\"]} sec')\n",
    "    return info\n",
    "\n",
    "\n",
    "def read_test(datapath, info):\n",
    "    # get test data\n",
    "    main_columns = info['tables']['main']\n",
    "    table_dtype = {key: TYPE_MAP[val] for key, val in main_columns.items()}\n",
    "\n",
    "    table_path = join(datapath, 'test', 'main_test.data')\n",
    "\n",
    "    date_list = [key for key, val in main_columns.items() if val == 'time']\n",
    "\n",
    "    test_data = pd.read_csv(\n",
    "        table_path, sep='\\t', dtype=table_dtype, parse_dates=date_list,\n",
    "        date_parser=lambda millisecs: millisecs if np.isnan(\n",
    "            float(millisecs)) else datetime.fromtimestamp(\n",
    "                float(millisecs) / 1000))\n",
    "    return test_data\n",
    "\n",
    "\n",
    "def write_predict(output_dir, dataname, prediction):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    prediction.rename('label', inplace=True)\n",
    "    prediction.to_csv(\n",
    "        join(output_dir, f'{dataname}.predict'), index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:46:53.077423Z",
     "start_time": "2019-05-31T08:46:52.688702Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  [05-31 16:46:52] Datanames: ['K', 'L']\n",
      "INFO  [05-31 16:46:52] Read data: K\n",
      "INFO  [05-31 16:46:52] Read info\n",
      "INFO  [05-31 16:46:52] Time budget for this task is 6000 sec\n",
      "INFO  [05-31 16:46:52] Table name: main\n",
      "INFO  [05-31 16:46:52] Table name: table_1\n",
      "INFO  [05-31 16:46:52] Table name: table_2\n",
      "INFO  [05-31 16:46:53] Table name: table_3\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = os.path.abspath('/Users/xijunli/Desktop/KDDCup2019/src/')\n",
    "DIRS = {\n",
    "        'input': join(ROOT_DIR, 'sample_data'),\n",
    "        'output': join(ROOT_DIR, 'sample_predictions'),\n",
    "        'program': join(ROOT_DIR, 'ingestion_program'),\n",
    "        'submission': join(ROOT_DIR, 'sample_code_submission_submit')\n",
    "}\n",
    "\n",
    "datanames = sorted(os.listdir(DIRS['input']))\n",
    "mprint(f'Datanames: {datanames}')\n",
    "timer = Timer()\n",
    "dataname = datanames[0]\n",
    "predictions = {}\n",
    "mprint(f'Read data: {dataname}')\n",
    "datapath = join(DIRS['input'], dataname)\n",
    "info = read_info(datapath)\n",
    "timer.set(info['time_budget'])\n",
    "train_data, train_label = read_train(datapath, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:46:53.092548Z",
     "start_time": "2019-05-31T08:46:53.079381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/xijunli/Desktop/KDDCup2019/src/sample_code_submission_submit\n"
     ]
    }
   ],
   "source": [
    "# os.chdir(os.path.abspath('..')+'/sample_code_submission/')\n",
    "os.chdir(DIRS[\"submission\"])\n",
    "print(os.path.abspath('.'))\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from automl import predict, train, validate\n",
    "from CONSTANT import MAIN_TABLE_NAME\n",
    "from merge import merge_table\n",
    "from preprocess import clean_df, clean_tables, feature_engineer\n",
    "from util import Config, log, show_dataframe, timeit\n",
    "\n",
    "config = Config(info)\n",
    "tables = copy.deepcopy(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:46:53.323510Z",
     "start_time": "2019-05-31T08:46:53.095224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning table main\n",
      "\n",
      "Start [clean_df]:\n",
      "----Start [fillna]:\n",
      "----End   [fillna]. Time elapsed: 0.02 sec.\n",
      "End   [clean_df]. Time elapsed: 0.02 sec.\n",
      "cleaning table table_1\n",
      "\n",
      "Start [clean_df]:\n",
      "----Start [fillna]:\n",
      "----End   [fillna]. Time elapsed: 0.14 sec.\n",
      "End   [clean_df]. Time elapsed: 0.14 sec.\n",
      "cleaning table table_2\n",
      "\n",
      "Start [clean_df]:\n",
      "----Start [fillna]:\n",
      "----End   [fillna]. Time elapsed: 0.04 sec.\n",
      "End   [clean_df]. Time elapsed: 0.04 sec.\n",
      "cleaning table table_3\n",
      "\n",
      "Start [clean_df]:\n",
      "----Start [fillna]:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xijunli/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/Users/xijunli/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/Users/xijunli/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/Users/xijunli/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----End   [fillna]. Time elapsed: 0.02 sec.\n",
      "End   [clean_df]. Time elapsed: 0.02 sec.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import CONSTANT\n",
    "from util import Config, Timer, log, timeit\n",
    "\n",
    "NUM_OP = [np.std, np.mean]\n",
    "\n",
    "for tname in tables:\n",
    "    log(f\"cleaning table {tname}\")\n",
    "    clean_df(tables[tname])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X = merge_table(Xs, self.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:46:53.401363Z",
     "start_time": "2019-05-31T08:46:53.326033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter main\n",
      "enter table_1\n",
      "leave table_1\n",
      "join main <--many_to_one--nt table_1\n",
      "\n",
      "Start [join]:\n",
      "End   [join]. Time elapsed: 0.03 sec.\n",
      "enter table_2\n",
      "leave table_2\n",
      "join main <--many_to_one--nt table_2\n",
      "\n",
      "Start [join]:\n",
      "End   [join]. Time elapsed: 0.02 sec.\n",
      "enter table_3\n",
      "leave table_3\n",
      "join main <--many_to_one--nt table_3\n",
      "\n",
      "Start [join]:\n",
      "End   [join]. Time elapsed: 0.01 sec.\n",
      "leave main\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import CONSTANT\n",
    "from util import Config, Timer, log, timeit\n",
    "from merge import bfs, dfs\n",
    "# def merge_table(tables, config):\n",
    "\n",
    "graph = defaultdict(list)\n",
    "for rel in config['relations']:\n",
    "    ta = rel['table_A']\n",
    "    tb = rel['table_B']\n",
    "    graph[ta].append({\n",
    "            \"to\": tb,\n",
    "            \"key\": rel['key'],\n",
    "            \"type\": rel['type']\n",
    "    })\n",
    "    graph[tb].append({\n",
    "            \"to\": ta,\n",
    "            \"key\": rel['key'],\n",
    "            \"type\": '_'.join(rel['type'].split('_')[::-1])\n",
    "    })\n",
    "bfs(CONSTANT.MAIN_TABLE_NAME, graph, config['tables'])\n",
    "X = dfs(CONSTANT.MAIN_TABLE_NAME, config, tables, graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Feature Engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T12:33:51.408721Z",
     "start_time": "2019-04-25T12:33:51.228162Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----Start [clean_df]:\n",
      "--------Start [fillna]:\n",
      "--------End   [fillna]. Time elapsed: 0.03 sec.\n",
      "----End   [clean_df]. Time elapsed: 0.03 sec.\n",
      "\n",
      "----Start [feature_engineer]:\n",
      "--------Start [transform_categorical_hash]:\n",
      "--------End   [transform_categorical_hash]. Time elapsed: 0.12 sec.\n",
      "\n",
      "--------Start [transform_datetime]:\n",
      "--------End   [transform_datetime]. Time elapsed: 0.01 sec.\n",
      "----End   [feature_engineer]. Time elapsed: 0.13 sec.\n"
     ]
    }
   ],
   "source": [
    "clean_df(X)\n",
    "X_fe = copy.deepcopy(X)\n",
    "feature_engineer(X_fe,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "left = pd.DataFrame({'key': ['K0', 'K1', 'K1', 'K3', 'K1', 'K1', 'K3'],\n",
    "                      'A': [1,2,3,4,5,6,7],\n",
    "                      'B': [3,5,2,5,2,3,2]})\n",
    " \n",
    "\n",
    "left_rolling = left.groupby([\"key\"]).rolling(3).agg({\"A\": \"sum\",\n",
    "                                     \"B\": \"sum\"}) \\\n",
    "    .reset_index(0, drop=True) \n",
    "\n",
    "left.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T12:31:16.757813Z",
     "start_time": "2019-04-25T12:31:16.754439Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "import hyperopt\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hyperopt import STATUS_OK, Trials, hp, space_eval, tpe\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from util import Config, log, timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T12:37:22.537165Z",
     "start_time": "2019-04-25T12:37:20.911393Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  7.65it/s, best loss: -0.6957329351390847]\n",
      "----auc = 0.6957 {'bagging_fraction': 0.8, 'bagging_freq': 22, 'feature_fraction': 0.5, 'learning_rate': 0.11643157545599682, 'max_depth': 6, 'min_child_weight': 9.195866326847561, 'num_leaves': 145, 'reg_alpha': 0.8629175589887947, 'reg_lambda': 1.935131592856683}\n"
     ]
    }
   ],
   "source": [
    "def data_sample(X: pd.DataFrame, y: pd.Series, nrows: int=5000):\n",
    "    # -> (pd.DataFrame, pd.Series):\n",
    "    if len(X) > nrows:\n",
    "        X_sample = X.sample(nrows, random_state=1)\n",
    "        y_sample = y[X_sample.index]\n",
    "    else:\n",
    "        X_sample = X\n",
    "        y_sample = y\n",
    "\n",
    "    return X_sample, y_sample\n",
    "\n",
    "def data_split(X: pd.DataFrame, y: pd.Series, test_size: float=0.2):\n",
    "    #  -> (pd.DataFrame, pd.Series, pd.DataFrame, pd.Series):\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=1)\n",
    "\n",
    "params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"auc\",\n",
    "        \"verbosity\": -1,\n",
    "        \"seed\": 1,\n",
    "        \"num_threads\": 4\n",
    "    }\n",
    "\n",
    "X_sample, y_sample = data_sample(X_fe, train_label, 30000)\n",
    "\n",
    "X_train, X_val, y_train, y_val = data_split(X_sample, y_sample, test_size=0.5)\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "valid_data = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "space = {\n",
    "        \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.01), np.log(0.5)),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", [-1, 2, 3, 4, 5, 6]),\n",
    "        \"num_leaves\": hp.choice(\"num_leaves\", np.linspace(10, 200, 50, dtype=int)),\n",
    "        \"feature_fraction\": hp.quniform(\"feature_fraction\", 0.5, 1.0, 0.1),\n",
    "        \"bagging_fraction\": hp.quniform(\"bagging_fraction\", 0.5, 1.0, 0.1),\n",
    "        \"bagging_freq\": hp.choice(\"bagging_freq\", np.linspace(0, 50, 10, dtype=int)),\n",
    "        \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 2),\n",
    "        \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 2),\n",
    "        \"min_child_weight\": hp.uniform('min_child_weight', 0.5, 10),\n",
    "}\n",
    "\n",
    "def objective(hyperparams):\n",
    "    model = lgb.train({**params, **hyperparams}, train_data, 300,\n",
    "                          valid_data, early_stopping_rounds=30, verbose_eval=0)\n",
    "\n",
    "    score = model.best_score[\"valid_0\"][params[\"metric\"]]\n",
    "        # in classification, less is better\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "trials = Trials()\n",
    "best = hyperopt.fmin(fn=objective, space=space, trials=trials,\n",
    "                         algo=tpe.suggest, max_evals=10, verbose=1,\n",
    "                         rstate=np.random.RandomState(1))\n",
    "\n",
    "hyperparams = space_eval(space, best)\n",
    "log(f\"auc = {-trials.best_trial['result']['loss']:0.4f} {hyperparams}\")\n",
    "\n",
    "\n",
    "# hyperparams = hyperopt_lightgbm(X_sample, y_sample, params, config)\n",
    "\n",
    "# X_train, X_val, y_train, y_val = data_split(X, y, 0.1)\n",
    "# train_data = lgb.Dataset(X_train, label=y_train)\n",
    "# valid_data = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "# config[\"model\"] = lgb.train({**params, **hyperparams},\n",
    "#                                 train_data,\n",
    "#                                 500,\n",
    "#                                 valid_data,\n",
    "#                                 early_stopping_rounds=30,\n",
    "#                                 verbose_eval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing feature tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T16:02:41.992842Z",
     "start_time": "2019-05-31T16:02:41.838758Z"
    }
   },
   "outputs": [],
   "source": [
    "import featuretools as ft\n",
    "\n",
    "X_nt = X.drop(columns=[\"t_01\"]).reset_index()\n",
    "\n",
    "es = ft.EntitySet(id=\"test\")\n",
    "es = es.entity_from_dataframe(entity_id='main',\n",
    "    dataframe=X_nt,\n",
    "    index=\"index\")\n",
    "#     variable_types={\n",
    "#         'type': ft.variable_types.Categorical\n",
    "#      })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T16:02:42.000196Z",
     "start_time": "2019-05-31T16:02:41.995092Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select the agg and trans primitives you want to look over\n",
    "agg_primitives=[\n",
    "        'std', 'min', 'max', 'mean', \n",
    "        'percent_true', 'last', 'count', \n",
    "        'trend', 'n_most_common'\n",
    "]\n",
    "trans_primitives=[\n",
    "        'percentile', 'cum_mean', 'cum_min', 'cum_count',\n",
    "        'subtract_numeric', 'add_numeric', 'diff', 'absolute',\n",
    "        'modulo_numeric', 'cum_max'\n",
    "        # 'hour', 'week', 'month', 'second', 'minute', 'weekday', 'year'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T16:03:26.210900Z",
     "start_time": "2019-05-31T16:02:42.002936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 5845 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xijunli/anaconda3/lib/python3.6/site-packages/distributed/bokeh/core.py:13: UserWarning: \n",
      "Dask needs bokeh >= 0.13.0 for the dashboard.\n",
      "Continuing without the dashboard.\n",
      "  warnings.warn(\"\\nDask needs bokeh >= 0.13.0 for the dashboard.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EntitySet scattered to workers in 4.648 seconds\n",
      "Elapsed: 00:34 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 11/11 chunks\n"
     ]
    }
   ],
   "source": [
    "features, feature_names = ft.dfs(\n",
    "    entityset=es,\n",
    "    target_entity='main',\n",
    "    agg_primitives=agg_primitives,\n",
    "    trans_primitives=trans_primitives,\n",
    "    max_depth=1,\n",
    "    n_jobs=-1,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-02T12:22:20.145693Z",
     "start_time": "2019-06-02T12:22:20.102111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_02\n",
      "c_01\n",
      "c_table_1.c_1\n",
      "c_table_1.c_2\n",
      "c_table_1.c_3\n",
      "c_table_1.c_4\n",
      "c_table_1.c_5\n",
      "c_table_1.c_6\n",
      "c_table_1.c_7\n",
      "c_table_2.c_1\n",
      "c_table_2.c_2\n",
      "c_table_2.c_3\n",
      "c_table_2.c_4\n",
      "c_table_2.c_5\n",
      "c_table_2.c_6\n"
     ]
    }
   ],
   "source": [
    "for c in features.columns:\n",
    "    if features[c].dtype == \"object\":\n",
    "        print(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
